import argparse
import os
import sys
import torch
from ultralytics import YOLO, settings

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.visualization import plot_training_metrics
from utils.file_utils import create_output_dirs, validate_files, ensure_model_extension, reorganize_training_outputs
from utils.metrics import generate_metrics_report, enhanced_metrics_analysis
from utils.per_class_evaluator import evaluate_and_visualize_per_class

# é…ç½®ultralyticså°†æ¨¡å‹ä¸‹è½½åˆ°modelsæ–‡ä»¶å¤¹ï¼Œæ•°æ®é›†ä½¿ç”¨å½“å‰ç›®å½•
settings.update({
    'weights_dir': 'models',
    'datasets_dir': 'datasets',
    'runs_dir': 'outputs/dentalai'  # è®¾ç½®è¿è¡Œè¾“å‡ºç›®å½•
})

def find_latest_checkpoint(output_dir):
    """
    åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒæ£€æŸ¥ç‚¹
    
    Args:
        output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        
    Returns:
        tuple: (checkpoint_path, training_dir) æˆ– (None, None)
    """
    if not os.path.exists(output_dir):
        return None, None
    
    # æŸ¥æ‰¾æ‰€æœ‰è®­ç»ƒç›®å½•ï¼ˆæŒ‰æ—¶é—´æˆ³å‘½åï¼‰
    train_dirs = []
    for item in os.listdir(output_dir):
        item_path = os.path.join(output_dir, item)
        if os.path.isdir(item_path) and item.startswith('train_'):
            train_dirs.append((item, item_path))
    
    if not train_dirs:
        return None, None
    
    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
    train_dirs.sort(key=lambda x: x[0], reverse=True)
    latest_dir = train_dirs[0][1]
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹æ–‡ä»¶
    possible_checkpoints = [
        os.path.join(latest_dir, "weights", "last.pt"),
        os.path.join(latest_dir, "temp_weights", "weights", "last.pt"),
        os.path.join(latest_dir, "temp_weights", "last.pt")
    ]
    
    for checkpoint in possible_checkpoints:
        if os.path.exists(checkpoint):
            return checkpoint, latest_dir
    
    return None, None


def setup_resume_training(args):
    """
    è®¾ç½®æ–­ç‚¹ç»­è®­ç›¸å…³çš„é…ç½®
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        tuple: (resume_path, output_dir, is_resuming)
    """
    is_resuming = False
    resume_path = None
    output_dir = None
    
    if args.resume or args.resume_dir:
        # å¤„ç†æŒ‡å®šç»­è®­ç›®å½•çš„æƒ…å†µ
        if args.resume_dir:
            if not os.path.exists(args.resume_dir):
                raise ValueError(f"æŒ‡å®šçš„ç»­è®­ç›®å½•ä¸å­˜åœ¨: {args.resume_dir}")
            
            # æŸ¥æ‰¾æ£€æŸ¥ç‚¹
            possible_checkpoints = [
                os.path.join(args.resume_dir, "weights", "last.pt"),
                os.path.join(args.resume_dir, "temp_weights", "weights", "last.pt"),
                os.path.join(args.resume_dir, "temp_weights", "last.pt")
            ]
            
            for checkpoint in possible_checkpoints:
                if os.path.exists(checkpoint):
                    resume_path = checkpoint
                    output_dir = args.resume_dir
                    is_resuming = True
                    break
            
            if not resume_path:
                raise ValueError(f"åœ¨æŒ‡å®šç›®å½•ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ£€æŸ¥ç‚¹æ–‡ä»¶: {args.resume_dir}")
        
        # å¤„ç†resumeå‚æ•°çš„æƒ…å†µ
        elif args.resume:
            if args.resume == "auto":
                # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹
                checkpoint, train_dir = find_latest_checkpoint(args.output_dir)
                if checkpoint:
                    resume_path = checkpoint
                    output_dir = train_dir
                    is_resuming = True
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°å¯ç»­è®­çš„æ£€æŸ¥ç‚¹ï¼Œå°†å¼€å§‹æ–°çš„è®­ç»ƒ")
            elif os.path.isfile(args.resume):
                # ç›´æ¥æŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶
                resume_path = args.resume
                # å°è¯•ä»æ£€æŸ¥ç‚¹è·¯å¾„æ¨æ–­è¾“å‡ºç›®å½•
                if "temp_weights" in resume_path:
                    output_dir = os.path.dirname(os.path.dirname(resume_path))
                else:
                    output_dir = os.path.dirname(os.path.dirname(resume_path))
                is_resuming = True
            else:
                raise ValueError(f"æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.resume}")
    
    return resume_path, output_dir, is_resuming


def detect_device_with_user_prompt():
    """
    æ™ºèƒ½è®¾å¤‡æ£€æµ‹å‡½æ•°ï¼Œè‡ªåŠ¨æ£€æµ‹GPUå¯ç”¨æ€§å¹¶ç»™å‡ºç”¨æˆ·å‹å¥½çš„æç¤º
    """
    print("ğŸ” æ­£åœ¨æ£€æµ‹å¯ç”¨çš„è®­ç»ƒè®¾å¤‡...")
    
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1024**3
        
        print(f"âœ… æ£€æµ‹åˆ° {device_count} ä¸ªå¯ç”¨ GPU")
        print(f"ğŸ¯ ä½¿ç”¨ GPU è®­ç»ƒ: {gpu_name}")
        print(f"ğŸ’¾ GPU æ˜¾å­˜: {gpu_memory}GB")
        return "0"
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨çš„ GPUï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒ")
        print("ğŸ’¡ æç¤º: CPU è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®:")
        print("   1. å®‰è£…æ”¯æŒ CUDA çš„ PyTorch:")
        print("      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        print("   2. ç¡®ä¿ NVIDIA æ˜¾å¡é©±åŠ¨å·²æ­£ç¡®å®‰è£…")
        print("   3. æ£€æŸ¥ CUDA ç‰ˆæœ¬å…¼å®¹æ€§")
        print("   4. å‚è€ƒå®˜æ–¹æ–‡æ¡£: https://pytorch.org/get-started/locally/")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­ CPU è®­ç»ƒ
        while True:
            try:
                user_input = input("ğŸ¤” æ˜¯å¦ç»§ç»­ä½¿ç”¨ CPU è®­ç»ƒ? (y/n): ").lower().strip()
                if user_input in ['y', 'yes', 'æ˜¯', '']:
                    print("ğŸ“ ç»§ç»­ä½¿ç”¨ CPU è®­ç»ƒ...")
                    return "cpu"
                elif user_input in ['n', 'no', 'å¦']:
                    print("ğŸšª å·²å–æ¶ˆè®­ç»ƒï¼Œè¯·é…ç½® GPU ç¯å¢ƒåé‡è¯•")
                    return None
                else:
                    print("â“ è¯·è¾“å…¥ y(æ˜¯) æˆ– n(å¦)")
            except (KeyboardInterrupt, EOFError):
                print("\nğŸšª è®­ç»ƒå·²å–æ¶ˆ")
                return None

def main():
    parser = argparse.ArgumentParser(
        description="YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python train.py                                      # é›¶é…ç½®è®­ç»ƒ(æ¨èæ–°æ‰‹)
  python train.py -e 100                               # ä»…æŒ‡å®šè®­ç»ƒè½®æ•°
  python train.py -m yolov8n -e 50                     # å°æ¨¡å‹å¿«é€Ÿè®­ç»ƒ
  python train.py -m yolov8s -e 100 -b 32              # ä¸­ç­‰è§„æ¨¡è®­ç»ƒ
  python train.py -m yolov8x -b -1 --device 0          # å¤§æ¨¡å‹è‡ªåŠ¨æ‰¹é‡å¤§å°
        """)
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--model', '-m', type=str, default="yolov8m",
                        help="æ¨¡å‹ç±»å‹: yolov8n, yolov8s, yolov8m, yolov8l, yolov8x (é»˜è®¤: yolov8m)")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', '-e', type=int, default=30,
                        help="è®­ç»ƒè½®æ•° (é»˜è®¤: 30)")
    parser.add_argument('--batch', '-b', type=int, default=16,
                        help="æ‰¹é‡å¤§å°: æ­£æ•´æ•°æˆ–-1(è‡ªåŠ¨æ‰¹é‡å¤§å°) (é»˜è®¤: 16)")
    parser.add_argument('--imgsz', type=int, default=640,
                        help="è¾“å…¥å›¾åƒå°ºå¯¸ (é»˜è®¤: 640)")
    parser.add_argument('--device', type=str, default="auto",
                        help="è®­ç»ƒè®¾å¤‡: auto, cpu, 0, 1, 2, 3... (é»˜è®¤: auto)")
    
    # æ•°æ®å’Œè¾“å‡º
    parser.add_argument('--data_dir', '-d', type=str, default="./preprocessed_datasets/dentalai",
                        help="è®­ç»ƒæ•°æ®æ–‡ä»¶å¤¹ï¼ŒåŒ…å« data.yaml (é»˜è®¤: ./preprocessed_datasets/dentalai)")
    parser.add_argument('--output_dir', '-o', type=str, default="./outputs/dentalai",
                        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./outputs/dentalai)")
    
    # è®­ç»ƒé€‰é¡¹
    parser.add_argument('--patience', type=int, default=30,
                        help="æ—©åœè€å¿ƒå€¼ï¼Œå¤šå°‘è½®æ— æ”¹å–„ååœæ­¢ (é»˜è®¤: 30)")
    parser.add_argument('--save_period', type=int, default=10,
                        help="ä¿å­˜æ£€æŸ¥ç‚¹çš„é—´éš”è½®æ•° (é»˜è®¤: 10)")
    
    # æ–­ç‚¹ç»­è®­é€‰é¡¹
    parser.add_argument('--resume', type=str, default=None,
                        help="æ–­ç‚¹ç»­è®­: 'auto' è‡ªåŠ¨ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤ï¼Œæˆ–æŒ‡å®šæ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument('--resume_dir', type=str, default=None,
                        help="æŒ‡å®šç»­è®­çš„è¾“å‡ºç›®å½•è·¯å¾„")
    
    # è¾“å‡ºæ§åˆ¶
    parser.add_argument('--nolog', action='store_true',
                        help="ç¦ç”¨æ—¥å¿—è¾“å‡ºå’Œå¯è§†åŒ–å›¾è¡¨")
    parser.add_argument('--verbose', action='store_true',
                        help="æ˜¾ç¤ºè¯¦ç»†è®­ç»ƒä¿¡æ¯")
    
    args = parser.parse_args()

    # å¤„ç†æ¨¡å‹æ–‡ä»¶å
    model_file = ensure_model_extension(args.model)
    
    # éªŒè¯æ‰¹é‡å¤§å°
    if args.batch <= 0 and args.batch != -1:
        raise ValueError("æ‰¹é‡å¤§å°å¿…é¡»ä¸ºæ­£æ•´æ•°æˆ–-1(è‡ªåŠ¨)")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§
    data_yaml = os.path.join(args.data_dir, "data.yaml")
    validate_files(model_file, data_yaml)

    # è®¾ç½®æ–­ç‚¹ç»­è®­
    resume_path, resume_output_dir, is_resuming = setup_resume_training(args)
    
    if is_resuming:
        print(f"ğŸ”„ æ£€æµ‹åˆ°ç»­è®­æ¨¡å¼")
        print(f"   ğŸ“ ç»­è®­ç›®å½•: {resume_output_dir}")
        print(f"   ğŸ’¾ æ£€æŸ¥ç‚¹æ–‡ä»¶: {resume_path}")
        base_dir = resume_output_dir
        dirs = {
            'base': base_dir,
            'weights': os.path.join(base_dir, "weights"),
            'logs': os.path.join(base_dir, "logs"),
            'logs_records': os.path.join(base_dir, "logs", "records"),
            'analysis': os.path.join(base_dir, "analysis"), 
            'meta': os.path.join(base_dir, "meta")
        }
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for dir_path in dirs.values():
            os.makedirs(dir_path, exist_ok=True)
    else:
        # åˆ›å»ºæ–°çš„è¾“å‡ºç›®å½•ç»“æ„
        dirs = create_output_dirs(
            args.model, args.epochs, args.output_dir, enable_logs=not args.nolog
        )

    # å‘åå…¼å®¹ï¼Œæå–ä¸»è¦ç›®å½•è·¯å¾„
    base_dir = dirs['base']
    weights_dir = dirs['weights']  # è¿™å°†æ˜¯ä¸´æ—¶çš„YOLOv8è¾“å‡ºç›®å½•

    # æ™ºèƒ½è®¾å¤‡æ£€æµ‹å’Œç”¨æˆ·æç¤º
    if args.device == "auto":
        device = detect_device_with_user_prompt()
        if device is None:  # ç”¨æˆ·é€‰æ‹©å–æ¶ˆè®­ç»ƒ
            return
    else:
        device = args.device
        print(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹")
    print(f"   ğŸ“¦ æ¨¡å‹: {model_file}")
    print(f"   ğŸ“Š è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"   ğŸ“ æ‰¹é‡å¤§å°: {args.batch}")
    print(f"   ğŸ–¼ï¸  å›¾åƒå°ºå¯¸: {args.imgsz}")
    print(f"   ğŸ’» è®­ç»ƒè®¾å¤‡: {device}")
    print(f"   ğŸ“ æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"   ğŸ’¾ è¾“å‡ºç›®å½•: {base_dir}")
    print(f"   ğŸ“ˆ æ—¥å¿—è®°å½•: {'å…³é—­' if args.nolog else 'å¼€å¯'}")

    # å¼€å§‹è®­ç»ƒ
    try:
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿æ¨¡å‹ä¸‹è½½åˆ°æ­£ç¡®ä½ç½®
        os.environ['YOLO_CONFIG_DIR'] = os.path.join(os.getcwd(), 'models')
        
        if is_resuming:
            print(f"ğŸ“‚ ä»æ£€æŸ¥ç‚¹ç»­è®­: {resume_path}")
            # ç»­è®­æ—¶ç›´æ¥åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶
            model = YOLO(resume_path)
        else:
            # æ­£å¸¸è®­ç»ƒæ—¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            model = YOLO(model_file)
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ!")
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
        if is_resuming:
            # ç»­è®­æ¨¡å¼ï¼šä»æ£€æŸ¥ç‚¹å¼€å§‹æ–°çš„è®­ç»ƒï¼ˆè¿ç§»å­¦ä¹ æ–¹å¼ï¼‰
            print(f"   ğŸ”„ ä½¿ç”¨æ£€æŸ¥ç‚¹æƒé‡è¿›è¡Œè¿ç§»è®­ç»ƒ")
            result = model.train(
                data=data_yaml,
                epochs=args.epochs,  # æ˜ç¡®æŒ‡å®šæ€»epochæ•°
                batch=args.batch,
                imgsz=args.imgsz,
                device=device,
                project=base_dir,
                name="temp_weights",
                exist_ok=True,
                patience=args.patience,
                save_period=args.save_period,
                verbose=args.verbose,
                amp=False
                # ä¸ä½¿ç”¨resumeå‚æ•°ï¼Œè€Œæ˜¯åŸºäºæ£€æŸ¥ç‚¹æƒé‡è¿›è¡Œæ–°çš„è®­ç»ƒ
            )
        else:
            # æ­£å¸¸è®­ç»ƒæ¨¡å¼
            result = model.train(
                data=data_yaml,
                epochs=args.epochs,
                batch=args.batch,
                imgsz=args.imgsz,
                device=device,  # ä½¿ç”¨æ™ºèƒ½æ£€æµ‹çš„è®¾å¤‡
                project=base_dir,  # ä½¿ç”¨base_dirä½œä¸ºä¸´æ—¶è®­ç»ƒç›®å½•
                name="temp_weights",  # ä¸´æ—¶åç§°
                exist_ok=True,
                patience=args.patience,
                save_period=args.save_period,
                verbose=args.verbose,
                amp=False  # ç¦ç”¨AMPä»¥é¿å…è‡ªåŠ¨ä¸‹è½½yolo11n.pt
            )
        
        # è®­ç»ƒå®Œæˆåé‡æ–°ç»„ç»‡è¾“å‡ºç»“æ„
        temp_weights_dir = os.path.join(base_dir, "temp_weights")
        
        if not args.nolog:
            # è¯»å–ç±»åˆ«åç§°
            import yaml
            try:
                with open(data_yaml, 'r', encoding='utf-8') as f:
                    data_config = yaml.safe_load(f)
                    class_names = data_config.get('names', ['Unknown'])
            except:
                class_names = ['Caries', 'Cavity', 'Crack', 'Tooth']  # é»˜è®¤ç±»åˆ«
            
            print("ğŸ“Š å¼€å§‹ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š...")
            
            # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šåˆ°ä¸´æ—¶logsç›®å½•
            temp_logs_dir = os.path.join(base_dir, "logs")
            os.makedirs(temp_logs_dir, exist_ok=True)
            
            # æ£€æŸ¥results.csvæ–‡ä»¶ä½ç½®
            results_csv = os.path.join(temp_weights_dir, "results.csv")
            
            if os.path.exists(results_csv):
                # 1. ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨
                metrics_plot_path = os.path.join(temp_logs_dir, "training_metrics.png")
                plot_training_metrics(results_csv, metrics_plot_path)
                
                # 2. è®¡ç®—ç»¼åˆæŒ‡æ ‡åˆ†æ
                metrics = enhanced_metrics_analysis(results_csv, class_names)
                
                # 3. ç”Ÿæˆè¯¦ç»†çš„æŒ‡æ ‡æŠ¥å‘Š
                report_path = os.path.join(temp_logs_dir, "metrics_report.md")
                generate_metrics_report(results_csv, class_names, report_path)
                
                # 4. è¿›è¡Œæ¯ç±»åˆ«è¯¦ç»†è¯„ä¼°
                best_model_path = os.path.join(temp_weights_dir, "weights", "best.pt")
                
                if os.path.exists(best_model_path):
                    print(f"ğŸ” å¼€å§‹æ¯ç±»åˆ«è¯¦ç»†æŒ‡æ ‡è¯„ä¼°... (ä½¿ç”¨: {best_model_path})")
                    per_class_metrics = evaluate_and_visualize_per_class(
                        best_model_path, data_yaml, class_names, temp_logs_dir
                    )
                    
                    # 5. ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æ•°æ®CSVæ–‡ä»¶
                    if per_class_metrics:
                        evaluation_csv_path = os.path.join(temp_logs_dir, "complete_evaluation_metrics.csv")
                        _save_complete_evaluation_csv(metrics, per_class_metrics, class_names, evaluation_csv_path)
                        print(f"ğŸ“‹ å®Œæ•´è¯„ä¼°æ•°æ®å·²ä¿å­˜è‡³: {evaluation_csv_path}")
                else:
                    per_class_metrics = None
                    print(f"âš ï¸ æœªæ‰¾åˆ°best.ptæ¨¡å‹æ–‡ä»¶: {best_model_path}")
                
                # 6. é‡æ–°ç»„ç»‡æ‰€æœ‰æ–‡ä»¶åˆ°æ–°ç»“æ„
                reorganize_training_outputs(temp_weights_dir, dirs, class_names)
                
                # 7. æ¸…ç†ä¸´æ—¶ç›®å½•
                if os.path.exists(temp_weights_dir):
                    import shutil
                    shutil.rmtree(temp_weights_dir)
                # æ³¨æ„ï¼šä¸åˆ é™¤temp_logs_dirï¼Œå› ä¸ºå®ƒå°±æ˜¯æˆ‘ä»¬çš„ç›®æ ‡logsç›®å½•
                
                print(f"âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å’Œå®Œæ•´è¯„ä¼°ç»“æœä¿å­˜è‡³: {base_dir}")
                print(f"ğŸ“Š æ–°çš„è¾“å‡ºç»“æ„:")
                print(f"   ğŸ“¦ æ¨¡å‹æ–‡ä»¶: {dirs['weights']}/")
                print(f"   ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹: {dirs['logs']}/")  
                print(f"   ğŸ“Š ç»“æœåˆ†æ: {dirs['analysis']}/")
                print(f"   âš™ï¸  è®­ç»ƒé…ç½®: {dirs['meta']}/")
                
                # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡æ‘˜è¦
                if 'metrics' in locals() and metrics:
                    print(f"ğŸ¯ å…³é”®æŒ‡æ ‡æ‘˜è¦:")
                    print(f"   - F1-Score: {metrics.get('f1_score', 0):.3f}")
                    print(f"   - Precision: {metrics.get('precision', 0):.3f}")
                    print(f"   - Recall: {metrics.get('recall', 0):.3f}")
                    print(f"   - mAP@0.5: {metrics.get('map50', 0):.3f}")
                    print(f"   - IoUè´¨é‡: {metrics.get('avg_iou_at_0.5', 0):.3f}")
                    
                # æ˜¾ç¤ºæ¯ç±»åˆ«F1-Scoreæ‘˜è¦
                if 'per_class_metrics' in locals() and per_class_metrics:
                    print(f"ğŸ† æ¯ç±»åˆ«F1-Score:")
                    for class_name, class_metrics in per_class_metrics.items():
                        print(f"   - {class_name}: {class_metrics.get('f1_score', 0):.3f}")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒç»“æœæ–‡ä»¶: {results_csv}")
                # è‡³å°‘é‡æ–°ç»„ç»‡åŸºæœ¬æ–‡ä»¶
                reorganize_training_outputs(temp_weights_dir, dirs, class_names)
                if os.path.exists(temp_weights_dir):
                    import shutil
                    shutil.rmtree(temp_weights_dir)
        else:
            # å³ä½¿ç¦ç”¨æ—¥å¿—ï¼Œä¹Ÿè¦é‡æ–°ç»„ç»‡åŸºæœ¬æ–‡ä»¶ç»“æ„
            reorganize_training_outputs(temp_weights_dir, dirs, [])
            if os.path.exists(temp_weights_dir):
                import shutil
                shutil.rmtree(temp_weights_dir)
            print(f"âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜è‡³: {base_dir}")
        
    except ConnectionError as e:
        print(f"âŒ ç½‘ç»œè¿æ¥é”™è¯¯: {e}")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°é¡¹ç›®ç›®å½•:")
        print(f"      https://github.com/ultralytics/assets/releases/download/v8.2.0/{model_file}")
        print("   3. æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹: python train.py -m yolov8n")
        return
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return


def _save_complete_evaluation_csv(overall_metrics, per_class_metrics, class_names, save_path):
    """
    ä¿å­˜å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡åˆ°CSVæ–‡ä»¶
    
    Args:
        overall_metrics (dict): æ•´ä½“æŒ‡æ ‡
        per_class_metrics (dict): æ¯ç±»åˆ«æŒ‡æ ‡
        class_names (list): ç±»åˆ«åç§°
        save_path (str): ä¿å­˜è·¯å¾„
    """
    try:
        import pandas as pd
        
        # å‡†å¤‡æ•°æ®
        data = []
        
        # æ·»åŠ æ•´ä½“æŒ‡æ ‡è¡Œ
        overall_row = {
            'Type': 'Overall',
            'Class': 'All',
            'Precision': overall_metrics.get('precision', 0),
            'Recall': overall_metrics.get('recall', 0),
            'F1_Score': overall_metrics.get('f1_score', 0),
            'mAP_50': overall_metrics.get('map50', 0),
            'mAP_50_95': overall_metrics.get('map50_95', 0),
            'IoU_Quality_50': overall_metrics.get('avg_iou_at_0.5', 0),
            'IoU_Quality_50_95': overall_metrics.get('avg_iou_0.5_to_0.95', 0),
            'Epoch': int(overall_metrics.get('epoch', 0))
        }
        data.append(overall_row)
        
        # æ·»åŠ æ¯ç±»åˆ«æŒ‡æ ‡è¡Œ
        if per_class_metrics:
            for class_name, class_metrics in per_class_metrics.items():
                class_row = {
                    'Type': 'Per_Class',
                    'Class': class_name,
                    'Precision': class_metrics.get('precision', 0),
                    'Recall': class_metrics.get('recall', 0),
                    'F1_Score': class_metrics.get('f1_score', 0),
                    'mAP_50': class_metrics.get('ap50', 0),
                    'mAP_50_95': class_metrics.get('ap50_95', 0),
                    'IoU_Quality_50': class_metrics.get('ap50', 0),  # AP50ä½œä¸ºIoU@0.5è´¨é‡æŒ‡æ ‡
                    'IoU_Quality_50_95': class_metrics.get('ap50_95', 0),  # AP50-95ä½œä¸ºç»¼åˆIoUè´¨é‡
                    'Epoch': int(overall_metrics.get('epoch', 0))
                }
                data.append(class_row)
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(data)
        df.to_csv(save_path, index=False, float_format='%.4f')
        
        print(f"[âœ“] å®Œæ•´è¯„ä¼°æŒ‡æ ‡CSVå·²ä¿å­˜: {save_path}")
        
    except Exception as e:
        print(f"[!] ä¿å­˜å®Œæ•´è¯„ä¼°CSVå¤±è´¥: {e}")

if __name__ == '__main__':
    main()
