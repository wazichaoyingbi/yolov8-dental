#!/usr/bin/env python3
"""
YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹æµ‹è¯•è„šæœ¬
åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½ï¼Œç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–ç»“æœ
"""

import argparse
import os
import sys
import random
import shutil
from pathlib import Path
import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib import rcParams
import pandas as pd
from ultralytics import YOLO
import yaml
from datetime import datetime

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.file_utils import create_output_dirs, validate_files
from utils.visualization import plot_training_metrics
from utils.metrics import generate_metrics_report, enhanced_metrics_analysis

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

def find_latest_model(output_dir):
    """
    åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°è®­ç»ƒçš„æ¨¡å‹
    
    Args:
        output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        
    Returns:
        str: æœ€æ–°æ¨¡å‹çš„è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
    """
    if not os.path.exists(output_dir):
        return None
    
    # æŸ¥æ‰¾æ‰€æœ‰è®­ç»ƒç›®å½•
    train_dirs = []
    for item in os.listdir(output_dir):
        item_path = os.path.join(output_dir, item)
        if os.path.isdir(item_path) and item.startswith('train_'):
            train_dirs.append((item, item_path))
    
    if not train_dirs:
        return None
    
    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
    train_dirs.sort(key=lambda x: x[0], reverse=True)
    
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶
    for _, train_dir in train_dirs:
        possible_models = [
            os.path.join(train_dir, "weights", "best.pt"),
            os.path.join(train_dir, "weights", "last.pt")
        ]
        
        for model_path in possible_models:
            if os.path.exists(model_path):
                return model_path
    
    return None

def load_ground_truth_labels(label_path):
    """
    åŠ è½½çœŸå®æ ‡ç­¾
    
    Args:
        label_path (str): æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        
    Returns:
        list: æ ‡ç­¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º[class_id, x_center, y_center, width, height]
    """
    labels = []
    if os.path.exists(label_path):
        with open(label_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 5:
                    labels.append([float(x) for x in parts])
    return labels

def denormalize_bbox(bbox, img_width, img_height):
    """
    å°†YOLOæ ¼å¼çš„å½’ä¸€åŒ–è¾¹ç•Œæ¡†è½¬æ¢ä¸ºåƒç´ åæ ‡
    
    Args:
        bbox (list): [x_center, y_center, width, height] (å½’ä¸€åŒ–)
        img_width (int): å›¾åƒå®½åº¦
        img_height (int): å›¾åƒé«˜åº¦
        
    Returns:
        tuple: (x_min, y_min, x_max, y_max) åƒç´ åæ ‡
    """
    x_center, y_center, width, height = bbox
    x_center *= img_width
    y_center *= img_height
    width *= img_width
    height *= img_height
    
    x_min = x_center - width / 2
    y_min = y_center - height / 2
    x_max = x_center + width / 2
    y_max = y_center + height / 2
    
    return int(x_min), int(y_min), int(x_max), int(y_max)

def visualize_predictions_vs_labels(image_paths, label_dir, model, class_names, output_dir, num_samples=10):
    """
    å¯è§†åŒ–é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾çš„å¯¹æ¯”
    
    Args:
        image_paths (list): å›¾åƒè·¯å¾„åˆ—è¡¨
        label_dir (str): æ ‡ç­¾ç›®å½•
        model: YOLOæ¨¡å‹
        class_names (list): ç±»åˆ«åç§°åˆ—è¡¨
        output_dir (str): è¾“å‡ºç›®å½•
        num_samples (int): é‡‡æ ·æ•°é‡
    """
    # éšæœºé€‰æ‹©å›¾åƒ
    selected_images = random.sample(image_paths, min(num_samples, len(image_paths)))
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 5, figsize=(25, 12))
    fig.suptitle('æµ‹è¯•é›†é¢„æµ‹ç»“æœå¯¹æ¯” (ç»¿è‰²: çœŸå®æ ‡ç­¾, çº¢è‰²: é¢„æµ‹ç»“æœ)', fontsize=16, fontweight='bold')
    
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
    
    for idx, image_path in enumerate(selected_images):
        row = idx // 5
        col = idx % 5
        ax = axes[row, col]
        
        # è¯»å–å›¾åƒ
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_height, img_width = image.shape[:2]
        
        # è·å–å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
        image_name = os.path.splitext(os.path.basename(image_path))[0]
        label_path = os.path.join(label_dir, f"{image_name}.txt")
        
        # åŠ è½½çœŸå®æ ‡ç­¾
        gt_labels = load_ground_truth_labels(label_path)
        
        # é¢„æµ‹
        results = model.predict(image_path, verbose=False)
        
        # æ˜¾ç¤ºå›¾åƒ
        ax.imshow(image)
        ax.set_title(f'å›¾ç‰‡ {idx + 1}: {os.path.basename(image_path)}', fontsize=10)
        ax.axis('off')
        
        # ç»˜åˆ¶çœŸå®æ ‡ç­¾ (ç»¿è‰²)
        for label in gt_labels:
            class_id, x_center, y_center, width, height = label
            x_min, y_min, x_max, y_max = denormalize_bbox(
                [x_center, y_center, width, height], img_width, img_height
            )
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            rect = patches.Rectangle(
                (x_min, y_min), x_max - x_min, y_max - y_min,
                linewidth=2, edgecolor='green', facecolor='none', alpha=0.8
            )
            ax.add_patch(rect)
            
            # æ·»åŠ ç±»åˆ«æ ‡ç­¾
            class_name = class_names[int(class_id)] if int(class_id) < len(class_names) else f'Class{int(class_id)}'
            ax.text(x_min, y_min - 5, f'GT: {class_name}', 
                   fontsize=8, color='green', fontweight='bold',
                   bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
        
        # ç»˜åˆ¶é¢„æµ‹ç»“æœ (çº¢è‰²)
        if results and len(results) > 0 and results[0].boxes is not None:
            boxes = results[0].boxes
            for box in boxes:
                # è·å–è¾¹ç•Œæ¡†åæ ‡
                x_min, y_min, x_max, y_max = box.xyxy[0].cpu().numpy()
                confidence = box.conf[0].cpu().numpy()
                class_id = int(box.cls[0].cpu().numpy())
                
                # åªæ˜¾ç¤ºç½®ä¿¡åº¦è¾ƒé«˜çš„é¢„æµ‹
                if confidence > 0.3:
                    # ç»˜åˆ¶è¾¹ç•Œæ¡†
                    rect = patches.Rectangle(
                        (x_min, y_min), x_max - x_min, y_max - y_min,
                        linewidth=2, edgecolor='red', facecolor='none', alpha=0.8
                    )
                    ax.add_patch(rect)
                    
                    # æ·»åŠ ç±»åˆ«æ ‡ç­¾å’Œç½®ä¿¡åº¦
                    class_name = class_names[class_id] if class_id < len(class_names) else f'Class{class_id}'
                    ax.text(x_min, y_max + 15, f'Pred: {class_name} ({confidence:.2f})', 
                           fontsize=8, color='red', fontweight='bold',
                           bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
    
    # ä¿å­˜å›¾åƒ
    plt.tight_layout()
    save_path = os.path.join(output_dir, 'test_predictions_comparison.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"[âœ“] é¢„æµ‹å¯¹æ¯”å¯è§†åŒ–å·²ä¿å­˜è‡³: {save_path}")

def plot_training_metrics(csv_path, output_path):
    """ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡å›¾è¡¨"""
    try:
        import pandas as pd
        
        # è¯»å–CSVæ•°æ®
        df = pd.read_csv(csv_path)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŒ‡æ ‡æ˜ å°„
        metrics_map = {
            'metrics/precision(B)': 'Precision',
            'metrics/recall(B)': 'Recall', 
            'metrics/mAP50(B)': 'mAP@0.5',
            'metrics/mAP50-95(B)': 'mAP@0.5:0.95'
        }
        
        epochs = df['epoch'] if 'epoch' in df.columns else range(len(df))
        
        for idx, (col_name, label) in enumerate(metrics_map.items()):
            row = idx // 2
            col = idx % 2
            
            if col_name in df.columns:
                axes[row, col].plot(epochs, df[col_name], marker='o', linewidth=2, markersize=4)
                axes[row, col].set_title(f'{label}', fontsize=12, fontweight='bold')
                axes[row, col].set_xlabel('Epoch')
                axes[row, col].set_ylabel(label)
                axes[row, col].grid(True, alpha=0.3)
                axes[row, col].set_ylim([0, 1])
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   [âœ“] æµ‹è¯•æŒ‡æ ‡å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
        
    except Exception as e:
        print(f"   âš ï¸ ç»˜åˆ¶æµ‹è¯•æŒ‡æ ‡å›¾è¡¨æ—¶å‡ºé”™: {e}")


def _save_complete_evaluation_csv(metrics, per_class_metrics, class_names, csv_path):
    """ä¿å­˜å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡åˆ°CSVæ–‡ä»¶"""
    import pandas as pd
    
    # åˆ›å»ºæ•´ä½“æŒ‡æ ‡æ•°æ®
    overall_data = {
        'ç±»åˆ«': ['Overall'],
        'ç²¾ç¡®ç‡': [metrics['precision']],
        'å¬å›ç‡': [metrics['recall']],
        'F1-Score': [metrics['f1_score']],
        'mAP@0.5': [metrics['map50']],
        'mAP@0.5:0.95': [metrics['map50_95']]
    }
    
    # æ·»åŠ æ¯ç±»åˆ«æŒ‡æ ‡
    if per_class_metrics:
        for i, class_name in enumerate(class_names):
            if i < len(per_class_metrics['precision']):
                overall_data['ç±»åˆ«'].append(class_name)
                overall_data['ç²¾ç¡®ç‡'].append(per_class_metrics['precision'][i])
                overall_data['å¬å›ç‡'].append(per_class_metrics['recall'][i])
                overall_data['F1-Score'].append(per_class_metrics['f1'][i])
                overall_data['mAP@0.5'].append(per_class_metrics['ap50'][i])
                overall_data['mAP@0.5:0.95'].append(per_class_metrics['ap'][i])
    
    # ä¿å­˜åˆ°CSV
    df = pd.DataFrame(overall_data)
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    

def run_test_evaluation(model_path, data_yaml, output_dir):
    """
    è¿è¡Œå®Œæ•´çš„æµ‹è¯•è¯„ä¼°
    
    Args:
        model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„
        data_yaml (str): æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        output_dir (str): è¾“å‡ºç›®å½•
        
    Returns:
        dict: è¯„ä¼°ç»“æœ
    """
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = YOLO(model_path)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    
    # è¯»å–æ•°æ®é…ç½®
    with open(data_yaml, 'r', encoding='utf-8') as f:
        data_config = yaml.safe_load(f)
    
    test_dir = data_config.get('test', 'test')
    class_names = data_config.get('names', ['Unknown'])
    
    print(f"ğŸ” å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    print(f"   ğŸ“‚ æµ‹è¯•æ•°æ®: {test_dir}")
    print(f"   ğŸ·ï¸  ç±»åˆ«æ•°é‡: {len(class_names)}")
    print(f"   ğŸ“Š ç±»åˆ«åç§°: {class_names}")
    
    # åˆ›å»ºåˆ†æç›®å½•
    analysis_dir = os.path.join(output_dir, 'analysis')
    os.makedirs(analysis_dir, exist_ok=True)
    
    # è¿è¡ŒéªŒè¯å¹¶ä¿å­˜è¯¦ç»†ç»“æœ
    results = model.val(
        data=data_yaml, 
        split='test', 
        save_json=True, 
        save_hybrid=True,
        plots=True,  # ç”Ÿæˆæ‰€æœ‰åˆ†æå›¾è¡¨
        save_dir=analysis_dir,  # ä¿å­˜åˆ°åˆ†æç›®å½•
        name='test_analysis'  # æŒ‡å®šå­ç›®å½•å
    )
    
    if results is None:
        print("âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥")
        return None
    
    # æå–å…³é”®æŒ‡æ ‡
    metrics = {
        'precision': float(results.box.mp),  # å¹³å‡ç²¾ç¡®ç‡
        'recall': float(results.box.mr),     # å¹³å‡å¬å›ç‡
        'map50': float(results.box.map50),   # mAP@0.5
        'map50_95': float(results.box.map),  # mAP@0.5:0.95
        'f1_score': 2 * float(results.box.mp) * float(results.box.mr) / (float(results.box.mp) + float(results.box.mr)) if (float(results.box.mp) + float(results.box.mr)) > 0 else 0,
    }
    
    print(f"ğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
    print(f"   - ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.4f}")
    print(f"   - å¬å›ç‡ (Recall): {metrics['recall']:.4f}")
    print(f"   - F1-Score: {metrics['f1_score']:.4f}")
    print(f"   - mAP@0.5: {metrics['map50']:.4f}")
    print(f"   - mAP@0.5:0.95: {metrics['map50_95']:.4f}")
    
    # å¤åˆ¶YOLOv8ç”Ÿæˆçš„åˆ†æå›¾è¡¨åˆ°ä¸»åˆ†æç›®å½•
    # YOLOv8ä¼šå°†ç»“æœä¿å­˜åˆ°outputsç›®å½•ä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å®é™…çš„ä¿å­˜ä½ç½®
    possible_yolo_dirs = [
        os.path.join(analysis_dir, 'test_analysis'),
        os.path.join('outputs', 'dentalai', 'detect', 'test_analysis'),
        os.path.join('outputs', 'dentalai', 'detect', 'test_analysis2'),
        os.path.join('outputs', 'dentalai', 'detect', 'test_analysis3'),
    ]
    
    yolo_analysis_dir = None
    for possible_dir in possible_yolo_dirs:
        if os.path.exists(possible_dir):
            yolo_analysis_dir = possible_dir
            break
    
    if yolo_analysis_dir and os.path.exists(yolo_analysis_dir):
        print(f"ğŸ“Š æ­£åœ¨æ•´ç†åˆ†æå›¾è¡¨...")
        print(f"   ä» {yolo_analysis_dir} å¤åˆ¶åˆ° {analysis_dir}")
        
        # éœ€è¦å¤åˆ¶çš„æ–‡ä»¶
        analysis_files = [
            'confusion_matrix.png',
            'confusion_matrix_normalized.png', 
            'BoxF1_curve.png',
            'BoxPR_curve.png',
            'BoxP_curve.png',
            'BoxR_curve.png',
            'val_batch0_labels.jpg',
            'val_batch0_pred.jpg'
        ]
        
        for file_name in analysis_files:
            src_file = os.path.join(yolo_analysis_dir, file_name)
            dst_file = os.path.join(analysis_dir, file_name)
            if os.path.exists(src_file):
                shutil.copy2(src_file, dst_file)
                print(f"   [âœ“] {file_name}")
            else:
                print(f"   [Ã—] {file_name} (æœªæ‰¾åˆ°)")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°YOLOv8ç”Ÿæˆçš„åˆ†æå›¾è¡¨ç›®å½•")
        
        # ç”Ÿæˆè®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–ï¼ˆå¦‚æœæœ‰results.csvï¼‰
        results_csv = os.path.join(yolo_analysis_dir, 'results.csv')
        if os.path.exists(results_csv):
            metrics_plot_path = os.path.join(analysis_dir, "test_metrics.png")
            plot_training_metrics(results_csv, metrics_plot_path)
            print(f"   [âœ“] test_metrics.png")
    
    # ç”Ÿæˆæ¯ç±»åˆ«è¯¦ç»†è¯„ä¼°ï¼ˆå¤ç”¨è®­ç»ƒæ—¶çš„åŠŸèƒ½ï¼‰
    print(f"ğŸ” å¼€å§‹æ¯ç±»åˆ«è¯¦ç»†æŒ‡æ ‡è¯„ä¼°...")
    try:
        from utils.per_class_evaluator import evaluate_and_visualize_per_class
        per_class_metrics = evaluate_and_visualize_per_class(
            model_path, data_yaml, class_names, analysis_dir, split='test'
        )
        
        if per_class_metrics:
            # ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æ•°æ®CSVæ–‡ä»¶
            evaluation_csv_path = os.path.join(output_dir, "complete_test_evaluation.csv")
            _save_complete_evaluation_csv(metrics, per_class_metrics, class_names, evaluation_csv_path)
            print(f"[âœ“] å®Œæ•´è¯„ä¼°æŒ‡æ ‡CSVå·²ä¿å­˜: {evaluation_csv_path}")
    except Exception as e:
        print(f"âš ï¸ æ¯ç±»åˆ«è¯„ä¼°å‡ºç°é—®é¢˜: {e}")
        per_class_metrics = None
    
    # è·å–æµ‹è¯•å›¾åƒåˆ—è¡¨è¿›è¡Œå¯è§†åŒ–å¯¹æ¯”
    base_dir = os.path.dirname(data_yaml)
    test_images_dir = os.path.join(base_dir, 'test', 'images')
    test_labels_dir = os.path.join(base_dir, 'test', 'labels')
    
    if os.path.exists(test_images_dir):
        image_paths = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:
            image_paths.extend(Path(test_images_dir).glob(f'*{ext}'))
            image_paths.extend(Path(test_images_dir).glob(f'*{ext.upper()}'))
        
        image_paths = [str(p) for p in image_paths]
        print(f"   ğŸ“¸ æµ‹è¯•å›¾åƒæ•°é‡: {len(image_paths)}")
        
        if len(image_paths) > 0:
            print(f"ğŸ¨ ç”Ÿæˆé¢„æµ‹å¯¹æ¯”å¯è§†åŒ–...")
            visualize_predictions_vs_labels(
                image_paths, test_labels_dir, model, class_names, analysis_dir
            )
    
    # ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŒ‡æ ‡æŠ¥å‘Š
    print(f"ğŸ“‹ ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŒ‡æ ‡æŠ¥å‘Š...")
    try:
        from utils.metrics import generate_metrics_report, enhanced_metrics_analysis
        
        # ä½¿ç”¨YOLOv8ç”Ÿæˆçš„results.csv
        results_csv = os.path.join(yolo_analysis_dir, 'results.csv') if os.path.exists(os.path.join(yolo_analysis_dir, 'results.csv')) else None
        
        if results_csv and os.path.exists(results_csv):
            # è®¡ç®—å¢å¼ºæŒ‡æ ‡åˆ†æ
            enhanced_metrics = enhanced_metrics_analysis(results_csv, class_names)
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_path = os.path.join(output_dir, "detailed_test_report.md") 
            generate_metrics_report(results_csv, class_names, report_path)
            print(f"[âœ“] è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°results.csvï¼Œè·³è¿‡è¯¦ç»†æŠ¥å‘Šç”Ÿæˆ")
            
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ—¶å‡ºç°é—®é¢˜: {e}")
    
    return metrics

def main():
    parser = argparse.ArgumentParser(
        description="YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹æµ‹è¯•è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python scripts/test.py                                    # ä½¿ç”¨é»˜è®¤å‚æ•°æµ‹è¯•
  python scripts/test.py -m ./outputs/train_xxx/weights/best.pt  # æŒ‡å®šæ¨¡å‹æ–‡ä»¶
  python scripts/test.py -d ./my_dataset                   # æŒ‡å®šæ•°æ®é›†ç›®å½•
  python scripts/test.py -o ./test_results                 # æŒ‡å®šè¾“å‡ºç›®å½•
        """)
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', '-m', type=str, default=None,
                        help="æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°è®­ç»ƒçš„æ¨¡å‹")
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data', '-d', type=str, default="./preprocessed_datasets/dentalai/data.yaml",
                        help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./preprocessed_datasets/dentalai/data.yaml)")
    parser.add_argument('--output_dir', '-o', type=str, default="./test_results",
                        help="æµ‹è¯•ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./test_results)")
    
    # å¯è§†åŒ–å‚æ•°
    parser.add_argument('--samples', '-s', type=int, default=10,
                        help="å¯è§†åŒ–å¯¹æ¯”çš„æ ·æœ¬æ•°é‡ (é»˜è®¤: 10)")
    parser.add_argument('--conf_threshold', '-c', type=float, default=0.3,
                        help="é¢„æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ (é»˜è®¤: 0.3)")
    
    args = parser.parse_args()
    
    # éªŒè¯æ•°æ®é…ç½®æ–‡ä»¶
    data_yaml = args.data
    if not os.path.exists(data_yaml):
        print(f"âŒ æ•°æ®é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_yaml}")
        return
    
    # ç¡®å®šæ¨¡å‹æ–‡ä»¶
    if args.model:
        model_path = args.model
        if not os.path.exists(model_path):
            print(f"âŒ æŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return
    else:
        # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
        print("ğŸ” æ­£åœ¨æŸ¥æ‰¾æœ€æ–°è®­ç»ƒçš„æ¨¡å‹...")
        model_path = find_latest_model("./outputs/dentalai")
        if not model_path:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶")
            print("ğŸ’¡ è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–ä½¿ç”¨ -m å‚æ•°æŒ‡å®šæ¨¡å‹æ–‡ä»¶è·¯å¾„")
            return
    
    print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
    timestamped_output_dir = os.path.join(args.output_dir, f"test_{timestamp}")
    os.makedirs(timestamped_output_dir, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)
    
    print(f"ğŸš€ å¼€å§‹æµ‹è¯• YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹")
    print(f"   ğŸ“¦ æ¨¡å‹æ–‡ä»¶: {model_path}")
    print(f"   ğŸ“ æ•°æ®é…ç½®: {data_yaml}")
    print(f"   ğŸ’¾ è¾“å‡ºç›®å½•: {timestamped_output_dir}")
    print(f"   ğŸ¨ å¯è§†åŒ–æ ·æœ¬: {args.samples}")
    print(f"   ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼: {args.conf_threshold}")
    
    try:
        # è¿è¡Œæµ‹è¯•è¯„ä¼°
        metrics = run_test_evaluation(model_path, data_yaml, timestamped_output_dir)
        
        if metrics:
            # ä¿å­˜æµ‹è¯•ç»“æœ
            results_file = os.path.join(timestamped_output_dir, 'test_results.json')
            import json
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            print(f"[âœ“] æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {results_file}")
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            report_file = os.path.join(timestamped_output_dir, 'test_report.md')
            generate_test_report(metrics, model_path, data_yaml, report_file)
            print(f"[âœ“] æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
            
            print(f"\nâœ… æµ‹è¯•å®Œæˆ! ç»“æœä¿å­˜è‡³: {timestamped_output_dir}")
            print(f"ğŸ“Š å…³é”®æŒ‡æ ‡:")
            print(f"   - F1-Score: {metrics['f1_score']:.4f}")
            print(f"   - ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
            print(f"   - å¬å›ç‡: {metrics['recall']:.4f}")
            print(f"   - mAP@0.5: {metrics['map50']:.4f}")
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def generate_test_report(metrics, model_path, data_yaml, report_path):
    """
    ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    
    Args:
        metrics (dict): æµ‹è¯•æŒ‡æ ‡
        model_path (str): æ¨¡å‹è·¯å¾„
        data_yaml (str): æ•°æ®é…ç½®è·¯å¾„
        report_path (str): æŠ¥å‘Šä¿å­˜è·¯å¾„
    """
    import datetime
    
    report_content = f"""# YOLOv8 ç‰™é½¿æ£€æµ‹æ¨¡å‹æµ‹è¯•æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **æµ‹è¯•æ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ¨¡å‹æ–‡ä»¶**: `{model_path}`
- **æ•°æ®é…ç½®**: `{data_yaml}`

## æµ‹è¯•ç»“æœ

### æ•´ä½“æ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| ç²¾ç¡®ç‡ (Precision) | {metrics['precision']:.4f} |
| å¬å›ç‡ (Recall) | {metrics['recall']:.4f} |
| F1-Score | {metrics['f1_score']:.4f} |
| mAP@0.5 | {metrics['map50']:.4f} |
| mAP@0.5:0.95 | {metrics['map50_95']:.4f} |

### æ€§èƒ½åˆ†æ

#### ğŸ“Š æŒ‡æ ‡è§£è¯»
- **ç²¾ç¡®ç‡ ({metrics['precision']:.4f})**: åœ¨æ‰€æœ‰é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹
- **å¬å›ç‡ ({metrics['recall']:.4f})**: åœ¨æ‰€æœ‰çœŸæ­£ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹  
- **F1-Score ({metrics['f1_score']:.4f})**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°ï¼Œç»¼åˆè¯„ä»·æŒ‡æ ‡
- **mAP@0.5 ({metrics['map50']:.4f})**: IoUé˜ˆå€¼ä¸º0.5æ—¶çš„å¹³å‡ç²¾åº¦
- **mAP@0.5:0.95 ({metrics['map50_95']:.4f})**: IoUé˜ˆå€¼ä»0.5åˆ°0.95çš„å¹³å‡ç²¾åº¦

#### ğŸ¯ æ€§èƒ½è¯„ä¼°
"""

    # æ€§èƒ½ç­‰çº§è¯„ä¼°
    f1_score = metrics['f1_score']
    if f1_score >= 0.9:
        performance_level = "ğŸ† ä¼˜ç§€"
    elif f1_score >= 0.8:
        performance_level = "ğŸ¥ˆ è‰¯å¥½"
    elif f1_score >= 0.7:
        performance_level = "ğŸ¥‰ ä¸€èˆ¬"
    else:
        performance_level = "âš ï¸ éœ€è¦æ”¹è¿›"
    
    report_content += f"- **æ•´ä½“æ€§èƒ½**: {performance_level} (F1-Score: {f1_score:.4f})\n"
    
    # æ·»åŠ å»ºè®®
    report_content += f"""
#### ğŸ’¡ ä¼˜åŒ–å»ºè®®
"""
    
    if metrics['precision'] > metrics['recall']:
        report_content += "- ç²¾ç¡®ç‡é«˜äºå¬å›ç‡ï¼Œæ¨¡å‹å€¾å‘äºä¿å®ˆé¢„æµ‹ï¼Œå¯è€ƒè™‘é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ä»¥æé«˜å¬å›ç‡\n"
    elif metrics['recall'] > metrics['precision']:
        report_content += "- å¬å›ç‡é«˜äºç²¾ç¡®ç‡ï¼Œæ¨¡å‹å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå¯è€ƒè™‘æé«˜ç½®ä¿¡åº¦é˜ˆå€¼æˆ–å¢åŠ æ­£åˆ™åŒ–\n"
    else:
        report_content += "- ç²¾ç¡®ç‡å’Œå¬å›ç‡è¾ƒä¸ºå¹³è¡¡ï¼Œæ¨¡å‹æ€§èƒ½ç¨³å®š\n"
    
    if f1_score < 0.7:
        report_content += "- F1-Scoreè¾ƒä½ï¼Œå»ºè®®:\n"
        report_content += "  - å¢åŠ è®­ç»ƒæ•°æ®é‡\n"
        report_content += "  - è°ƒæ•´æ¨¡å‹æ¶æ„æˆ–è¶…å‚æ•°\n"
        report_content += "  - æ£€æŸ¥æ•°æ®è´¨é‡å’Œæ ‡æ³¨å‡†ç¡®æ€§\n"
    
    report_content += """
## å¯è§†åŒ–ç»“æœ

æµ‹è¯•è¿‡ç¨‹ä¸­ç”Ÿæˆäº†ä»¥ä¸‹å¯è§†åŒ–ç»“æœï¼š
- `test_predictions_comparison.png`: éšæœºé€‰å–10å¼ å›¾ç‰‡çš„é¢„æµ‹ç»“æœå¯¹æ¯”

## æ–‡ä»¶è¯´æ˜

- `test_results.json`: è¯¦ç»†çš„æ•°å€¼ç»“æœ
- `test_report.md`: æœ¬æµ‹è¯•æŠ¥å‘Š  
- `test_predictions_comparison.png`: é¢„æµ‹ç»“æœå¯è§†åŒ–

---
*æŠ¥å‘Šç”± YOLOv8 ç‰™é½¿æ£€æµ‹æµ‹è¯•è„šæœ¬è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)

if __name__ == '__main__':
    main()
