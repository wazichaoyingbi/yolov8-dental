"""
Per-class metrics calculator for YOLOv8 evaluation
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from ultralytics import YOLO
import os
import json


class PerClassMetrics:
    """
    YOLOv8 æ¯ç±»åˆ«æŒ‡æ ‡è®¡ç®—å™¨
    """
    
    def __init__(self, model_path, data_yaml_path, class_names):
        """
        åˆå§‹åŒ–æ¯ç±»åˆ«æŒ‡æ ‡è®¡ç®—å™¨
        
        Args:
            model_path (str): è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            data_yaml_path (str): æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
            class_names (list): ç±»åˆ«åç§°åˆ—è¡¨
        """
        self.model_path = model_path
        self.data_yaml_path = data_yaml_path
        self.class_names = class_names
        self.model = None
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            self.model = YOLO(self.model_path)
            print(f"[âœ“] æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            return True
        except Exception as e:
            print(f"[!] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def evaluate_per_class(self, split='val', output_dir=None):
        """
        è®¡ç®—æ¯ç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        
        Args:
            split (str): æ•°æ®åˆ†å‰² ('train', 'val', 'test')
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„ï¼Œå¦‚æœæŒ‡å®šåˆ™å°†éªŒè¯ç»“æœä¿å­˜åˆ°æ­¤ç›®å½•
        
        Returns:
            dict: æ¯ç±»åˆ«æŒ‡æ ‡ç»“æœ
        """
        if not self.model:
            if not self.load_model():
                return {}
        
        try:
            # è¿è¡ŒéªŒè¯è¯„ä¼°ï¼ˆç¦ç”¨æ–‡ä»¶è¾“å‡ºï¼Œé¿å…é‡å¤ç”Ÿæˆï¼‰
            print(f"[ğŸ”„] æ­£åœ¨è¯„ä¼° {split} æ•°æ®é›†...")
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºYOLOv8è¾“å‡ºï¼Œå®Œæˆååˆ é™¤
            import tempfile
            import shutil
            
            with tempfile.TemporaryDirectory() as temp_dir:
                # åªæå–æŒ‡æ ‡æ•°æ®ï¼Œè¾“å‡ºåˆ°ä¸´æ—¶ç›®å½•
                results = self.model.val(
                    data=self.data_yaml_path, 
                    split=split, 
                    verbose=False,
                    save=False,      # ç¦ç”¨ä¿å­˜å›¾ç‰‡
                    plots=False,     # ç¦ç”¨ç”Ÿæˆå›¾è¡¨
                    save_json=False, # ç¦ç”¨ä¿å­˜JSON
                    save_txt=False,  # ç¦ç”¨ä¿å­˜æ ‡ç­¾
                    project=temp_dir,  # è¾“å‡ºåˆ°ä¸´æ—¶ç›®å½•
                    name='temp_val',   # ä¸´æ—¶éªŒè¯åç§°
                    exist_ok=True
                )
                # ä¸´æ—¶ç›®å½•ä¼šåœ¨withå—ç»“æŸæ—¶è‡ªåŠ¨åˆ é™¤
            
            # æå–æ¯ç±»åˆ«æŒ‡æ ‡
            per_class_metrics = {}
            
            if hasattr(results, 'box') and results.box is not None:
                # å®‰å…¨åœ°æå–æ•°ç»„æ•°æ®çš„è¾…åŠ©å‡½æ•°
                def safe_extract_array(attr_value):
                    """å®‰å…¨æå–æ•°ç»„æ•°æ®ï¼Œå¤„ç†tensorå’Œnumpyæ•°ç»„"""
                    if attr_value is None:
                        return None
                    if hasattr(attr_value, 'cpu'):  # PyTorch tensor
                        return attr_value.cpu().numpy()
                    elif hasattr(attr_value, 'numpy'):  # æŸäº›ç±»å‹çš„æ•°ç»„
                        return attr_value.numpy()
                    else:  # å·²ç»æ˜¯numpyæ•°ç»„
                        return attr_value
                
                # AP@0.5 æ¯ç±»åˆ«
                ap50_per_class = None
                if hasattr(results.box, 'ap50') and results.box.ap50 is not None:
                    ap50_per_class = safe_extract_array(results.box.ap50)
                    
                # AP@0.5:0.95 æ¯ç±»åˆ«  
                ap_per_class = None
                if hasattr(results.box, 'ap') and results.box.ap is not None:
                    ap_per_class = safe_extract_array(results.box.ap)
                    
                # ç²¾ç¡®ç‡æ¯ç±»åˆ«
                precision_per_class = None
                if hasattr(results.box, 'p') and results.box.p is not None:
                    precision_per_class = safe_extract_array(results.box.p)
                    
                # å¬å›ç‡æ¯ç±»åˆ«
                recall_per_class = None
                if hasattr(results.box, 'r') and results.box.r is not None:
                    recall_per_class = safe_extract_array(results.box.r)
                
                # ç»„ç»‡æ¯ç±»åˆ«æ•°æ®
                for i, class_name in enumerate(self.class_names):
                    # åˆå§‹åŒ–æŒ‡æ ‡å­—å…¸
                    per_class_metrics[class_name] = {
                        'precision': 0.0,
                        'recall': 0.0,
                        'ap50': 0.0,
                        'ap50_95': 0.0,
                        'f1_score': 0.0
                    }
                    
                    # å®‰å…¨åœ°æå–å„é¡¹æŒ‡æ ‡
                    if precision_per_class is not None and i < len(precision_per_class):
                        per_class_metrics[class_name]['precision'] = float(precision_per_class[i])
                        
                    if recall_per_class is not None and i < len(recall_per_class):
                        per_class_metrics[class_name]['recall'] = float(recall_per_class[i])
                        
                    if ap50_per_class is not None and i < len(ap50_per_class):
                        per_class_metrics[class_name]['ap50'] = float(ap50_per_class[i])
                        
                    if ap_per_class is not None and i < len(ap_per_class):
                        # ap_per_class[i] å¯èƒ½æ˜¯å•ä¸ªå€¼æˆ–æ•°ç»„
                        ap_val = ap_per_class[i]
                        if hasattr(ap_val, '__len__') and len(ap_val) > 0:
                            per_class_metrics[class_name]['ap50_95'] = float(np.mean(ap_val))
                        else:
                            per_class_metrics[class_name]['ap50_95'] = float(ap_val) if not np.isnan(ap_val) else 0.0
                        
                    # è®¡ç®—F1-Score
                    p = per_class_metrics[class_name]['precision']
                    r = per_class_metrics[class_name]['recall']
                    if p + r > 0:
                        per_class_metrics[class_name]['f1_score'] = 2 * p * r / (p + r)
            
            return per_class_metrics
            
        except Exception as e:
            print(f"[!] æ¯ç±»åˆ«æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}
    
    def plot_per_class_metrics(self, metrics_dict, save_path):
        """
        ç»˜åˆ¶æ¯ç±»åˆ«æŒ‡æ ‡å¯¹æ¯”å›¾
        
        Args:
            metrics_dict (dict): æ¯ç±»åˆ«æŒ‡æ ‡å­—å…¸
            save_path (str): ä¿å­˜è·¯å¾„
        """
        try:
            if not metrics_dict:
                print("[!] æ²¡æœ‰å¯ç”¨çš„æ¯ç±»åˆ«æŒ‡æ ‡æ•°æ®")
                return
            
            # å‡†å¤‡æ•°æ®
            classes = list(metrics_dict.keys())
            precision_vals = [metrics_dict[c]['precision'] for c in classes]
            recall_vals = [metrics_dict[c]['recall'] for c in classes]
            f1_vals = [metrics_dict[c]['f1_score'] for c in classes]
            ap50_vals = [metrics_dict[c]['ap50'] for c in classes]
            ap50_95_vals = [metrics_dict[c]['ap50_95'] for c in classes]
            
            # åˆ›å»º2x2å­å›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Per-Class Metrics Analysis', fontsize=16, fontweight='bold')
            
            # 1. ç²¾ç¡®ç‡å’Œå¬å›ç‡å¯¹æ¯”
            ax1 = axes[0, 0]
            x = np.arange(len(classes))
            width = 0.35
            ax1.bar(x - width/2, precision_vals, width, label='Precision', color='green', alpha=0.7)
            ax1.bar(x + width/2, recall_vals, width, label='Recall', color='red', alpha=0.7)
            ax1.set_xlabel('Classes')
            ax1.set_ylabel('Score')
            ax1.set_title('Precision vs Recall by Class')
            ax1.set_xticks(x)
            ax1.set_xticklabels(classes, rotation=45)
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # 2. F1-Scoreå¯¹æ¯”
            ax2 = axes[0, 1]
            bars = ax2.bar(classes, f1_vals, color='purple', alpha=0.7)
            ax2.set_xlabel('Classes')
            ax2.set_ylabel('F1-Score')
            ax2.set_title('F1-Score by Class')
            ax2.set_xticklabels(classes, rotation=45)
            ax2.grid(True, alpha=0.3)
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, f1_vals):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{val:.3f}', ha='center', va='bottom')
            
            # 3. mAP@0.5å¯¹æ¯”
            ax3 = axes[1, 0]
            bars = ax3.bar(classes, ap50_vals, color='blue', alpha=0.7)
            ax3.set_xlabel('Classes')
            ax3.set_ylabel('mAP@0.5')
            ax3.set_title('mAP@0.5 by Class')
            ax3.set_xticklabels(classes, rotation=45)
            ax3.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, ap50_vals):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height,
                        f'{val:.3f}', ha='center', va='bottom')
            
            # 4. mAP@0.5:0.95å¯¹æ¯”
            ax4 = axes[1, 1]
            bars = ax4.bar(classes, ap50_95_vals, color='orange', alpha=0.7)
            ax4.set_xlabel('Classes')
            ax4.set_ylabel('mAP@0.5:0.95')
            ax4.set_title('mAP@0.5:0.95 by Class')
            ax4.set_xticklabels(classes, rotation=45)
            ax4.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, ap50_95_vals):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height,
                        f'{val:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"[âœ“] æ¯ç±»åˆ«æŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
            
        except Exception as e:
            print(f"[!] ç»˜åˆ¶æ¯ç±»åˆ«æŒ‡æ ‡å›¾è¡¨å¤±è´¥: {e}")
    
    def save_per_class_report(self, metrics_dict, save_path):
        """
        ä¿å­˜æ¯ç±»åˆ«æŒ‡æ ‡è¯¦ç»†æŠ¥å‘Š
        
        Args:
            metrics_dict (dict): æ¯ç±»åˆ«æŒ‡æ ‡å­—å…¸
            save_path (str): ä¿å­˜è·¯å¾„
        """
        try:
            if not metrics_dict:
                print("[!] æ²¡æœ‰å¯ç”¨çš„æ¯ç±»åˆ«æŒ‡æ ‡æ•°æ®")
                return
            
            report = "# æ¯ç±»åˆ«è¯¦ç»†æŒ‡æ ‡æŠ¥å‘Š\n\n"
            report += "## æŒ‡æ ‡æ¦‚è§ˆ\n\n"
            
            # åˆ›å»ºè¡¨æ ¼
            report += "| ç±»åˆ« | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1-Score | mAP@0.5 | mAP@0.5:0.95 |\n"
            report += "|------|--------|--------|----------|---------|---------------|\n"
            
            total_precision = 0
            total_recall = 0
            total_f1 = 0
            total_ap50 = 0
            total_ap50_95 = 0
            
            for class_name, metrics in metrics_dict.items():
                report += f"| {class_name} | {metrics['precision']:.4f} | {metrics['recall']:.4f} | {metrics['f1_score']:.4f} | {metrics['ap50']:.4f} | {metrics['ap50_95']:.4f} |\n"
                
                total_precision += metrics['precision']
                total_recall += metrics['recall'] 
                total_f1 += metrics['f1_score']
                total_ap50 += metrics['ap50']
                total_ap50_95 += metrics['ap50_95']
            
            # å¹³å‡å€¼
            num_classes = len(metrics_dict)
            avg_precision = total_precision / num_classes
            avg_recall = total_recall / num_classes
            avg_f1 = total_f1 / num_classes
            avg_ap50 = total_ap50 / num_classes
            avg_ap50_95 = total_ap50_95 / num_classes
            
            report += f"| **å¹³å‡å€¼** | **{avg_precision:.4f}** | **{avg_recall:.4f}** | **{avg_f1:.4f}** | **{avg_ap50:.4f}** | **{avg_ap50_95:.4f}** |\n\n"
            
            # è¯¦ç»†åˆ†æ
            report += "## è¯¦ç»†åˆ†æ\n\n"
            
            # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®è¡¨ç°çš„ç±»åˆ«
            best_f1_class = max(metrics_dict.items(), key=lambda x: x[1]['f1_score'])
            worst_f1_class = min(metrics_dict.items(), key=lambda x: x[1]['f1_score'])
            
            report += f"### ğŸ† æœ€ä½³F1-Scoreç±»åˆ«\n"
            report += f"**{best_f1_class[0]}**: F1-Score = {best_f1_class[1]['f1_score']:.4f}\n\n"
            
            report += f"### âš ï¸ æœ€å·®F1-Scoreç±»åˆ«\n"
            report += f"**{worst_f1_class[0]}**: F1-Score = {worst_f1_class[1]['f1_score']:.4f}\n\n"
            
            # æ€§èƒ½å»ºè®®
            report += "### ğŸ’¡ æ”¹è¿›å»ºè®®\n\n"
            for class_name, metrics in metrics_dict.items():
                if metrics['f1_score'] < 0.5:
                    report += f"- **{class_name}**: F1-Scoreè¾ƒä½({metrics['f1_score']:.3f})ï¼Œå»ºè®®å¢åŠ è¯¥ç±»åˆ«çš„è®­ç»ƒæ ·æœ¬æˆ–è°ƒæ•´æ•°æ®å¢å¼ºç­–ç•¥\n"
                if metrics['precision'] < 0.6:
                    report += f"- **{class_name}**: ç²¾ç¡®ç‡è¾ƒä½({metrics['precision']:.3f})ï¼Œå¯èƒ½å­˜åœ¨è¾ƒå¤šè¯¯æ£€ï¼Œå»ºè®®æé«˜ç½®ä¿¡åº¦é˜ˆå€¼\n"
                if metrics['recall'] < 0.6:
                    report += f"- **{class_name}**: å¬å›ç‡è¾ƒä½({metrics['recall']:.3f})ï¼Œå¯èƒ½å­˜åœ¨è¾ƒå¤šæ¼æ£€ï¼Œå»ºè®®é™ä½ç½®ä¿¡åº¦é˜ˆå€¼æˆ–å¢åŠ è®­ç»ƒæ•°æ®\n"
            
            report += f"\n---\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
            
            # ä¿å­˜æŠ¥å‘Š
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            print(f"[âœ“] æ¯ç±»åˆ«æŒ‡æ ‡æŠ¥å‘Šå·²ä¿å­˜è‡³: {save_path}")
            
        except Exception as e:
            print(f"[!] ä¿å­˜æ¯ç±»åˆ«æŒ‡æ ‡æŠ¥å‘Šå¤±è´¥: {e}")


def evaluate_and_visualize_per_class(model_path, data_yaml_path, class_names, output_dir):
    """
    ä¾¿æ·å‡½æ•°ï¼šè¯„ä¼°å¹¶å¯è§†åŒ–æ¯ç±»åˆ«æŒ‡æ ‡
    
    Args:
        model_path (str): æ¨¡å‹è·¯å¾„
        data_yaml_path (str): æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        class_names (list): ç±»åˆ«åç§°åˆ—è¡¨
        output_dir (str): è¾“å‡ºç›®å½•
    """
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = PerClassMetrics(model_path, data_yaml_path, class_names)
        
        # è¯„ä¼°éªŒè¯é›†ï¼ˆåªæå–æŒ‡æ ‡ï¼Œä¸ç”Ÿæˆé‡å¤æ–‡ä»¶ï¼‰
        print("[ğŸ”„] å¼€å§‹æ¯ç±»åˆ«æŒ‡æ ‡è¯„ä¼°...")
        metrics = evaluator.evaluate_per_class('val')
        
        if not metrics:
            print("[!] æ¯ç±»åˆ«æŒ‡æ ‡è¯„ä¼°å¤±è´¥")
            return
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        plot_path = os.path.join(output_dir, "per_class_metrics.png")
        evaluator.plot_per_class_metrics(metrics, plot_path)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_path = os.path.join(output_dir, "per_class_report.md")
        evaluator.save_per_class_report(metrics, report_path)
        
        print(f"[âœ…] æ¯ç±»åˆ«æŒ‡æ ‡è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨: {plot_path}")
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
        return metrics
        
    except Exception as e:
        print(f"[!] æ¯ç±»åˆ«æŒ‡æ ‡è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}")
        return None
